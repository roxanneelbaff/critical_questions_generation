{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running command: python eval_scripts/evaluation.py --metric similarity --input_path data_splits/validation.json --submission_path output/elbaff_experiment/output_llama8b_social_n2_Teo_Srd.json --threshold 0.6\n",
      "Distribution of the labels: Counter({'Useful': 351, 'not_able_to_evaluate': 123, 'Unhelpful': 60, 'Invalid': 24})\n",
      "Distribution of the intervention punctuation: Counter({0.6666666666666666: 74, 1.0: 55, 0.3333333333333333: 38, 0: 19})\n",
      "Overall punctuation 0.6290322580645163\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from social_agents.agent_builder import SocialAgentBuilder\n",
    "from social_agents.utils import eval_experiment\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import os\n",
    "all_results_path = SocialAgentBuilder.ROOT_FOLDER + \"experiment_results.csv\"\n",
    "path_ = SocialAgentBuilder.ROOT_FOLDER + \"output_*.json\"\n",
    "time_log_path = SocialAgentBuilder.ROOT_FOLDER + \"time_log.csv\"\n",
    "\n",
    "out_files = [x for x in glob(path_) if x.find(\"_eval_\") == -1]\n",
    "evaluated_files = [x for x in glob(path_) if x.find(\"_eval_\") > -1]\n",
    "\n",
    "data_split = \"validation\"\n",
    "threshold = 0.6\n",
    "metric = \"similarity\"\n",
    "\n",
    "\n",
    "all_results = []\n",
    "for out_ in out_files:\n",
    "    eval_name = out_.replace(\"json\", f\"_eval_{metric}_{str(threshold).replace('.', '')}.json\")\n",
    "    if eval_name in evaluated_files:\n",
    "        print(\"already evaluated\")\n",
    "        continue\n",
    "    \n",
    "    exp_name = os.path.basename(out_).replace(\"output_\", \"\").replace(\".json\", \"\")\n",
    "    eval_dict = {\"experiment_name\": exp_name}\n",
    "    eval_dict = eval_dict | eval_experiment(submission_path=out_, data_split=data_split, threshold=threshold)\n",
    "\n",
    "    time_log_df = pd.read_csv(time_log_path)\n",
    "    if exp_name not in time_log_df.columns:\n",
    "        print(f\"time not logged for {exp_name}\")\n",
    "    else:\n",
    "        eval_dict[\"time_mean\"] = time_log_df[exp_name].mean()\n",
    "        eval_dict[\"time_std\"] = time_log_df[exp_name].std()\n",
    "\n",
    "    try:\n",
    "        all_experiments_results_df = pd.read_csv(all_results_path)\n",
    "        all_experiments_results_df = all_experiments_results_df.drop(columns=[col for col in all_experiments_results_df.columns if col.startswith('Unnamed')])\n",
    "    except FileNotFoundError:\n",
    "        all_experiments_results_df = pd.DataFrame()\n",
    "    \n",
    "\n",
    "    all_experiments_results_df = pd.concat([all_experiments_results_df, pd.DataFrame([eval_dict])], ignore_index=True)\n",
    "    all_experiments_results_df.to_csv(all_results_path, index=False)\n",
    "    all_results.append(eval_dict)\n",
    "    break\n",
    "\n",
    "new_results_df = pd.DataFrame(all_results)\n",
    "all_experiments_results_df = pd.read_csv(all_results_path)\n",
    "summary_df = all_experiments_results_df[[\"experiment_name\", \"Useful_ratio\", \"3/3_ratio\", \"overall_punctuation\", \"time_mean\", \"time_std\"]]\n",
    "summary_df.to_csv(all_results_path.replace(\".csv\", \"_summary.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_experiments_results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob(\"output/elbaff_experiment/final_states/*.json\")\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "fail_fs = []\n",
    "numbers = []\n",
    "correct_n=[]\n",
    "for f in files:\n",
    "    with open(f, \"r\") as f_:\n",
    "        s = json.load(f_)\n",
    "        if len(s['final_cq']['critical_questions']) != 3:\n",
    "            numbers.append(len(s['final_cq']['critical_questions']))\n",
    "            fail_fs.append(f)\n",
    "        else:\n",
    "            correct_n.append(len(s['final_cq']['critical_questions']))\n",
    "\n",
    "\n",
    "Counter(numbers).values()\n",
    "#Counter(correct_n)\n",
    "fail_fs = list(set(fail_fs))\n",
    "len(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for ffail in fail_fs:\n",
    "    if os.path.exists(ffail):\n",
    "        os.remove(ffail)\n",
    "    else:\n",
    "        print(\"Doesnt exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted = fail_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(deleted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
