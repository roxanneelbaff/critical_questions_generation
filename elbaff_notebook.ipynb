{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Task: Critical thinking generation\n",
    "## Loadind Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q social_agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show social_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import social_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINTON_1_1\n",
      "CLINTON: \"The central question in this election is really what kind of country we want to be and what kind of future we 'll build together\n",
      "Today is my granddaughter 's second birthday\n",
      "I think about this a lot\n",
      "we have to build an economy that works for everyone , not just those at the top\n",
      "we need new jobs , good jobs , with rising incomes\n",
      "I want us to invest in you\n",
      "I want us to invest in your future\n",
      "jobs in infrastructure , in advanced manufacturing , innovation and technology , clean , renewable energy , and small business\n",
      "most of the new jobs will come from small business\n",
      "We also have to make the economy fairer\n",
      "That starts with raising the national minimum wage and also guarantee , finally , equal pay for women 's work\n",
      "I also want to see more companies do profit-sharing\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "# Reading the data\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from social_agents.utils import get_st_data\n",
    "\n",
    "\n",
    "for key, line in tqdm.tqdm(get_st_data(\"sample\").items()):\n",
    "    print(key)\n",
    "\n",
    "    print(line['intervention'])\n",
    "    input_text = line['intervention']\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os, getpass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "env, LLM, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")\n",
    "_set_env(\"LANGCHAIN_API_KEY\")\n",
    "_set_env(\"MISTRAL_API_KEY\")\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"shared_task_critical_questions\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temperature = 0 not for o3-mini\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. Zero Shot LLM to start with o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from social_agents.graph_tools import BasicCQModel\n",
    "model_name = \"o3-mini\"\n",
    "\n",
    "basic_agent = BasicCQModel(llm_name = model_name, temperature=None ) #interrupt_before=['human_feedback'], checkpointer=memory)\n",
    "\n",
    "\n",
    "display(Image(basic_agent.graph.get_graph(xray=1).draw_mermaid_png()))\n",
    "\n",
    "# RUN\n",
    "# basic_agent.run_experiment()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "### Evaulate \n",
    "\n",
    "- Run shell \n",
    "\n",
    "```shell\n",
    "\n",
    "python3 eval_scripts/evaluation.py \\\n",
    "    --metric similarity \\\n",
    "    --input_path data_splits/validation.json \\\n",
    "    --submission_path output/output_o3-mini_temperatureNA.json \\\n",
    "    --threshold 0.6 \n",
    "\n",
    "```\n",
    "\n",
    "- OUTPUT : output/output_o3-mini_temperatureNA._eval_similarity_06.json\n",
    "\n",
    "\n",
    "\n",
    "**Overall count**\n",
    "\n",
    "| **Questions Labels** |  **#**  | **ratio** |\n",
    "|:--------------------|-------:|---------:|\n",
    "| useful               | **329** |  **0,59** |\n",
    "| unhelpful            |      63 |      0,11 |\n",
    "| Invalid              |       8 |      0,01 |\n",
    "| Not able to evaluate |     158 |      0,28 |\n",
    "| **Total**            | **558** |         1 |\n",
    "\n",
    "\n",
    "**Overall count within each argument**\n",
    "\n",
    "| **n/3 useful questions per arg** | **# of arguments** | **ratio** |\n",
    "|:-------------------------------:|-------------------:|:---------:|\n",
    "|                               0/3 |                 17 |      0,10 |\n",
    "|                               1/3 |                 51 |      0,27 |\n",
    "|                               2/3 |             **76** |      0,40 |\n",
    "|                               3/3 |                 42 |      0,23 |\n",
    "| **Total**                       |            **186** |         1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import operator\n",
    "from typing import Annotated, Dict, Literal, TypedDict\n",
    "\n",
    "from social_agents.graph_tools import CQSTAbstractAgent\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from social_agents.objects import  CriticalQuestionList\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "class Confirmation(BaseModel):\n",
    "    confirmation: str = Field(\n",
    "        description=\"The LLM confirms the role assigned to it by simply saying ok.\"\n",
    "    )\n",
    "\n",
    "def _add_to_dict(left: dict | None, right: dict | None) -> dict:\n",
    "    \"\"\"\n",
    "    Merge two dictionaries with list values.\n",
    "    \n",
    "    If a key exists in both dictionaries, append the elements from the right \n",
    "    dictionary's list to the left dictionary's list. If either input is None, \n",
    "    it's treated as an empty dictionary.\n",
    "    \n",
    "    Args:\n",
    "        left (dict or None): The first dictionary. If None, treated as {}.\n",
    "        right (dict or None): The second dictionary. If None, treated as {}.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the merged lists for each key.\n",
    "              For example, merging {'key1': ['e1', 'e2']} with {'key1': ['e_new']}\n",
    "              will yield {'key1': ['e1', 'e2', 'e_new']}.\n",
    "    \"\"\"\n",
    "    left = left or {}\n",
    "    right = right or {}\n",
    "    merged = left.copy()  # Start with a copy of left\n",
    "    \n",
    "    \n",
    "    for key, right_value in right.items():\n",
    "        if not isinstance(right_value, list):\n",
    "            right_value = [right_value]\n",
    "\n",
    "        if key in merged:\n",
    "            merged[key] = merged[key] + right_value\n",
    "        else:\n",
    "            merged[key] = right_value\n",
    "            \n",
    "    return merged\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "class SocialAgentAnswer(BaseModel):\n",
    "    critical_question_list: CriticalQuestionList = Field(\n",
    "        description=\"The list of all the critical questions and their ranks to criticize and reveal the weaknesses of an argument.\"\n",
    "    )\n",
    "    question_type: Literal[\"debate\", \"reflect\", \"question\"]\n",
    "    prompt: str\n",
    "\n",
    "\n",
    "class SocialAgentState(TypedDict):\n",
    "    input_arg: str\n",
    "    collaborative_strategy: list\n",
    "    current_round: int = -1 \n",
    "    #round_answer_dict: Dict[str, Annotated[list[SocialAgentAnswer], operator.add]]\n",
    "    round_answer_dict: Annotated[Dict[str, list[SocialAgentAnswer]], _add_to_dict]\n",
    "\n",
    "    # OutPut\n",
    "    final_cq: CriticalQuestionList\n",
    "\n",
    "\n",
    "collaborative_strategy = [\"debate\"]#[\"debate\", \"reflect\", \"debate\"]\n",
    "agent_trait_lst: list = [\"easy_going\", \"overconfident\", \"easy_going\"]\n",
    "llm_name = \"gpt-4o-mini-2024-07-18\"# \"o3-mini-2025-01-31\"\n",
    "temperature = 0.75\n",
    "llm_lst = [\n",
    "    CQSTAbstractAgent._init_llm(llm_name, temperature)\n",
    "    for _ in range(len(agent_trait_lst))\n",
    "]\n",
    "validator_llm = CQSTAbstractAgent._init_llm(llm_name, temperature)\n",
    "\n",
    "\n",
    "def llm_role_node(state: SocialAgentState):\n",
    "    are_all_roles_confirmed = True\n",
    "    for i, llm in enumerate(llm_lst):\n",
    "        structured_llm = llm.with_structured_output(Confirmation)\n",
    "        with open(f\"prompts/trait_{agent_trait_lst[i]}.txt\", \"r\") as f:\n",
    "            trait_prompt = f.read()\n",
    "\n",
    "        response = structured_llm.invoke(\n",
    "            trait_prompt\n",
    "        )\n",
    "        if response.confirmation.lower() == \"ok\":\n",
    "            print(f\"role {agent_trait_lst[i]} confirmed\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"Unconfirmed Role: {response.confirmation}\")\n",
    "            are_all_roles_confirmed = False\n",
    "            break\n",
    "            \n",
    "    return {\"role_confirmed\": are_all_roles_confirmed,\n",
    "            \"current_round\": -1}\n",
    "\n",
    "def is_role_confirmed(state):\n",
    "    if state[\"role_confirmed\"]:\n",
    "        return \"question_node\"\n",
    "    else: return END\n",
    "\n",
    "def question_node(state: SocialAgentState):\n",
    "    round_answer_dict = {}\n",
    "    for i, llm in enumerate(llm_lst):\n",
    "        structured_llm = llm.with_structured_output(CriticalQuestionList)\n",
    "        # with open(\"prompts/system.txt\", \"r\") as f:\n",
    "        #    system_prompt = f.read()\n",
    "        with open(\"prompts/question.txt\", \"r\") as file:\n",
    "            instructions = file.read()\n",
    "\n",
    "        instructions = instructions.format(\n",
    "            input_arg=state[\"input_arg\"],\n",
    "        )\n",
    "        response = structured_llm.invoke(instructions)\n",
    "        answer: SocialAgentAnswer = SocialAgentAnswer(\n",
    "            critical_question_list=response,\n",
    "            question_type=\"question\",\n",
    "            prompt=instructions,\n",
    "        )\n",
    "        round_answer_dict[f\"agent{i}\"] = [answer]\n",
    "\n",
    "    return {\"current_round\": state[\"current_round\"] + 1,\n",
    "        \"round_answer_dict\":round_answer_dict }\n",
    "\n",
    "\n",
    "def debate_node(state: SocialAgentState):\n",
    "    answer_round= {}\n",
    "    for i, llm in enumerate(llm_lst):\n",
    "        structured_llm = llm.with_structured_output(CriticalQuestionList)\n",
    "        print(\"ROLE {i} remember:\", llm.invoke(\"Do you remember what is you ROLE? \"))\n",
    "        print(\"arg {i} :\", llm.invoke(\"Can you show the argument that I asked you to evaluate? \"))\n",
    "\n",
    "        with open(\"prompts/strategy_debate.txt\", \"r\") as file:\n",
    "            instructions = file.read()\n",
    "\n",
    "        # Get others answer\n",
    "        round_answer_dict = state[\"round_answer_dict\"]\n",
    "        print(round_answer_dict)\n",
    "        others_answers = [\n",
    "            round_answer_dict[f\"agent{x}\"][-1] for x, _ in enumerate(llm_lst)\n",
    "        ]\n",
    "\n",
    "        other_agents_response_str = \"\"\n",
    "        other_cq_str = \"\\n- critical question {id}: '{cq}', reasoning: '{reason}'.\\n\"\n",
    "        for a_num, other_ in enumerate(others_answers):\n",
    "            if a_num == i: continue\n",
    "            other_agents_response_str += f\"Agent{a_num+1}:\"\n",
    "            for cq in other_.critical_question_list.critical_questions:\n",
    "                other_agents_response_str = (\n",
    "                    other_agents_response_str\n",
    "                    + other_cq_str.format(id = cq.id, cq=cq.critical_question, reason=cq.reason)\n",
    "                )\n",
    "            other_agents_response_str += \"\\n\"\n",
    "\n",
    "        instructions = instructions.format(\n",
    "            input_arg=state[\"input_arg\"],\n",
    "            other_agents_response=other_agents_response_str,\n",
    "        )\n",
    "\n",
    "        response = structured_llm.invoke(instructions)\n",
    "        answer: SocialAgentAnswer = SocialAgentAnswer(\n",
    "            critical_question_list=response,\n",
    "            question_type=\"debate\",\n",
    "            prompt=instructions,\n",
    "        )\n",
    "\n",
    "        #round_answer_dict[f\"agent{i}\"].append(answer)\n",
    "        answer_round[f\"agent{i}\"] = [answer]\n",
    "        #state[\"round_answer_dict\"] = round_answer_dict\n",
    "\n",
    "    \n",
    "    return {\"current_round\": state[\"current_round\"] + 1,\n",
    "            \"round_answer_dict\":answer_round }\n",
    "    \n",
    "    \n",
    "    \n",
    "def decide_next(state) -> Literal[\"debate\", \"reflect\", \"validate\"]:\n",
    "    if len(state[\"collaborative_strategy\"]) > state[\"current_round\"]:\n",
    "        print(f'Moving to state: {state[\"collaborative_strategy\"][state[\"current_round\"]]}')\n",
    "        return f'{state[\"collaborative_strategy\"][state[\"current_round\"]]}_node'\n",
    "    else:\n",
    "        print(\"Moving to state: validate_note\")\n",
    "        return \"validate_node\"\n",
    "\n",
    "\n",
    "\n",
    "def reflect_node(state: SocialAgentState):\n",
    "    answer_round={}\n",
    "    for i, llm in enumerate(llm_lst):\n",
    "        structured_llm = llm.with_structured_output(CriticalQuestionList)\n",
    "\n",
    "        with open(\"prompts/strategy_reflect.txt\", \"r\") as file:\n",
    "            instructions = file.read()\n",
    "\n",
    "        # Get others answer\n",
    "        round_answer_dict = state[\"round_answer_dict\"]\n",
    "        previous_answer = round_answer_dict[f\"agent{i}\"][-1]\n",
    "        instructions = instructions.format(\n",
    "            input_arg=state[\"input_arg\"], previous_answer=previous_answer\n",
    "        )\n",
    "\n",
    "        response = structured_llm.invoke(instructions)\n",
    "        answer: SocialAgentAnswer = SocialAgentAnswer(\n",
    "            critical_question_list=response,\n",
    "            question_type=\"reflect\",\n",
    "            prompt=instructions,\n",
    "        )\n",
    "\n",
    "        #round_answer_dict[f\"agent{i}\"].append(answer)\n",
    "        answer_round[f\"agent{i}\"] = [answer]\n",
    "        #state[\"round_answer_dict\"] = round_answer_dict\n",
    "        \n",
    "    return {\"current_round\": state[\"current_round\"] + 1,\n",
    "        \"round_answer_dict\":answer_round }\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def validate_node(state: SocialAgentState):\n",
    "    structured_llm = validator_llm.with_structured_output(CriticalQuestionList)\n",
    "    with open(\"prompts/validator.txt\", \"r\") as file:\n",
    "        instructions = file.read()\n",
    "    # Get others answer\n",
    "    round_answer_dict = state[\"round_answer_dict\"]\n",
    "    others_answers = [\n",
    "        round_answer_dict[f\"agent{x}\"][-1] for x, _ in enumerate(llm_lst)\n",
    "    ]\n",
    "    other_agents_response_str = \"\"\n",
    "    other_cq_str = \"\\n- critical question {id}: '{cq}'.\\n\" # reasoning: '{reason}'\n",
    "    for a_num, other_ in enumerate(others_answers):\n",
    "        other_agents_response_str += f\"Agent{a_num+1}:\"\n",
    "        for cq in other_.critical_question_list.critical_questions:\n",
    "            other_agents_response_str = (\n",
    "                other_agents_response_str\n",
    "                + other_cq_str.format(id = cq.id, cq=cq.critical_question, reason=cq.reason)\n",
    "            )\n",
    "        other_agents_response_str += \"\\n\\n\"\n",
    "    instructions = instructions.format(\n",
    "        input_arg=state[\"input_arg\"],\n",
    "        other_agents_response=other_agents_response_str,\n",
    "    )\n",
    "    response = structured_llm.invoke(instructions)\n",
    "    print(instructions)\n",
    "    return {\"final_cq\": response}\n",
    "\n",
    "\n",
    "\n",
    "builder = StateGraph(SocialAgentState)\n",
    "\n",
    "builder.add_node(\"llm_role_node\", llm_role_node)\n",
    "builder.add_node(\"question_node\", question_node)\n",
    "builder.add_node(\"debate_node\", debate_node)\n",
    "builder.add_node(\"reflect_node\", reflect_node)\n",
    "builder.add_node(\"validate_node\", validate_node)\n",
    "\n",
    "builder.add_edge(START, \"llm_role_node\")\n",
    "builder.add_edge(\"llm_role_node\", \"question_node\")\n",
    "builder.add_conditional_edges(\"question_node\", decide_next, [\"debate_node\", \"reflect_node\", \"validate_node\"])\n",
    "builder.add_conditional_edges(\"debate_node\", decide_next, [\"debate_node\", \"reflect_node\", \"validate_node\"])\n",
    "builder.add_conditional_edges(\"reflect_node\", decide_next, [\"debate_node\", \"reflect_node\", \"validate_node\"])\n",
    "builder.add_edge(\"validate_node\", END)\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.runnables.graph import CurveStyle\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile( checkpointer=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_ = [[]]\n",
    "a_[0] = [\"2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role easy_going confirmed\n",
      "role overconfident confirmed\n",
      "role easy_going confirmed\n",
      "Moving to state: validate_note\n",
      "You are an expert critical thinker, skilled at evaluating questions that challenge and expose blind spots in arguments.\n",
      "\n",
      "You will be provided with an argument from an oral debate, along with nine generated critical questions.\n",
      "\n",
      "# ARGUMENT\n",
      "\n",
      "Speaker: I am human therefore I am mortal\n",
      "\n",
      "# CRITICAL QUESTIONS\n",
      "\n",
      "Agent1:\n",
      "- critical question 0: 'Does being human inherently imply mortality, or could there be exceptions or different interpretations of what it means to be human?'.\n",
      "\n",
      "- critical question 1: 'What evidence supports the claim that all humans are mortal, and are there any counterexamples to this claim?'.\n",
      "\n",
      "- critical question 2: 'Is the reasoning that 'I am human therefore I am mortal' based on an oversimplification of the relationship between identity and existence?'.\n",
      "\n",
      "\n",
      "Agent2:\n",
      "- critical question 0: 'What evidence supports the claim that all humans are mortal, and can this be applied universally without exceptions?'.\n",
      "\n",
      "- critical question 1: 'Is it possible to conceive of a being that is both human and not mortal, and how does this affect the validity of the argument?'.\n",
      "\n",
      "- critical question 2: 'How does the premise that 'I am human' automatically lead to the conclusion 'therefore I am mortal'? Is there a logical necessity linking these two statements?'.\n",
      "\n",
      "\n",
      "Agent3:\n",
      "- critical question 0: 'What evidence supports the claim that all humans are mortal?'.\n",
      "\n",
      "- critical question 1: 'Does being human necessarily entail mortality, or could there be exceptions?'.\n",
      "\n",
      "- critical question 2: 'Is there a difference between being human and having human-like qualities that might not include mortality?'.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Read the responses carefully and select the top three critical questions based on the following criteria:\n",
      "\n",
      "   - A question must not introduce a new concept or topic that are not part of the original argument.\n",
      "   - A question must not employ flawed reasoning (e.g., it criticizes positions or claims that the speaker does not hold).\n",
      "   - A question must not be generic, but rather specific to the argument.\n",
      "\n",
      "Select the three best questions that meet these standards.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "response = graph.invoke({\"collaborative_strategy\": collaborative_strategy, \"input_arg\": \"I am human therefore I am mortal\"}, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['agent0', 'agent1', 'agent2'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['final_cq']\n",
    "response['round_answer_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'critical_question_list': {'critical_questions': [{'id': 0,\n",
       "    'critical_question': 'What evidence supports the claim that all humans are mortal, and how does this relate to the definition of being human?',\n",
       "    'reason': 'This question challenges the foundational assumption that being human inherently means being mortal, seeking empirical evidence for this claim.'},\n",
       "   {'id': 1,\n",
       "    'critical_question': 'Are there any exceptions to the assertion that all humans are mortal, such as potential advancements in medicine or technology?',\n",
       "    'reason': 'This question probes for potential counterexamples or exceptions to the mortality claim, revealing whether the argument holds under scrutiny.'},\n",
       "   {'id': 2,\n",
       "    'critical_question': 'Does the argument assume that the nature of humanity is solely defined by mortality, and are there other philosophical or existential perspectives that could challenge this view?',\n",
       "    'reason': 'This question examines the philosophical implications of defining humanity strictly in terms of mortality, prompting a discussion on whether this is a comprehensive understanding of being human.'}]},\n",
       " 'question_type': 'question',\n",
       " 'prompt': '\\n# CONTEXT\\n\\nThe task of Critical Questions Generation consists of generating useful critical questions when given an argumentative text. For this purpose, you will be given an argument from a real debate, generate a set of critical Questions to reflect if it is acceptable or fallacious. \\n\\n\\n# INPUT\\n\\nI am human therefore I am mortal\\n\\nRead the given argument and do the following:\\n\\n- Generate 3 *CRITICAL QUESTIONS* to unmask the assumptions held by the premises of the argument and attack its inference. Your goal is to unravel the acceptability level and fallacies in an argument.\\n- Make sure to keep your questions specific to the original argument\\n- avoid generic questions that can be asked to any argument.\\n'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_out = response['round_answer_dict']['agent2']\n",
    "len(ag_out)\n",
    "ag_out[0].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ag_out = response['round_answer_dict']['agent0']\n",
    "len(ag_out)\n",
    "#ag_out[1].model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png(curve_style=CurveStyle.LINEAR, padding=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CriticalQuestionList(critical_questions=[CriticalQuestion(id=0, critical_question=\"Does the premise 'I am human' necessarily imply mortality, or could there be exceptions to this rule?\", reason='This question directly challenges the assumption that being human equates to being mortal, which is central to the argument.'), CriticalQuestion(id=1, critical_question='What specific characteristics of being human lead to the conclusion of mortality, and are these characteristics universally accepted?', reason='This question seeks to clarify the basis of the argument and whether the characteristics used to define mortality among humans are universally acknowledged.'), CriticalQuestion(id=2, critical_question='Does the argument assume that the nature of humanity is solely defined by mortality, and are there other philosophical or existential perspectives that could challenge this view?', reason=\"This question explores whether the argument's conclusion hinges solely on mortality and invites consideration of alternative views on what it means to be human.\")])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['final_cq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
